syntax = "proto3";
option py_generic_services = false;
package specdecode;

// Request to start generation with prompt and max tokens.
message StartRequest {
  string prompt = 1;
  uint32 max_new_tokens = 2;
}

message StartResponse {
  bool acknowledged = 1;
}

// Single-token generation request/response (not used in speculative decoding).
message GenerateRequest {
  // Reserved for future use.
}
message GenerateResponse {
  int32 token_id = 1;
}

// **Legacy single-token verification (optional)** 
message VerifyRequest {
  int32 draft_token_id = 1;
}
message VerifyResponse {
  bool match = 1;
  int32 correct_token_id = 2;
}

// **New multi-token (chunk) verification** 
message VerifyChunkRequest {
  repeated int32 draft_tokens = 1;
}
message VerifyChunkResponse {
  bool all_matched = 1;          // True if the entire chunk matches
  uint32 match_count = 2;        // Number of tokens from the start that matched
  int32 correct_token = 3;       // Targetâ€™s token at first mismatch (0 if none)
  bool finished = 4;             // True if target model reached EOS in this chunk
}

// Speculative Decoding gRPC Service
service SpeculativeService {
  rpc StartGeneration(StartRequest) returns (StartResponse);
  rpc VerifyDraftToken(VerifyRequest) returns (VerifyResponse);       // legacy
  rpc VerifyDraftChunk(VerifyChunkRequest) returns (VerifyChunkResponse);  // new
  rpc GenerateFull(StartRequest) returns (GenerateResponse);
}