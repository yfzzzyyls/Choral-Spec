syntax = "proto3";
option py_generic_services = false;

package specdecode;

// Request to start generation with prompt and maximum tokens.
message StartRequest {
  uint64 session_id = 1;       // added to track sessions
  string prompt = 2;
  uint32 max_new_tokens = 3;
  // pass gamma to the server
  uint32 gamma = 4;
}

message StartResponse {
  bool acknowledged = 1;
}

// Request for speculative draft token verification (multi-token).
message VerifyRequest {
  uint64 session_id = 1;
  repeated int32 draft_tokens = 2;
}

// Response from target model containing probabilities for draft tokens and finish flag.
message VerifyResponse {
  repeated float target_probs = 1;
  bool finished = 2;
}

// Request to finalize tokens after verification. Contains number of draft tokens accepted.
message FinalizeRequest {
  uint64 session_id = 1;
  uint32 accepted_count = 2;
  // how many total tokens were in the chunk so we know if it was fully accepted
  uint32 draft_chunk_size = 3;
}

// Response after finalizing tokens. Contains the token generated by target if a rejection occurred 
// (or 0 if none), and whether the target model has finished generation.
message FinalizeResponse {
  int32 final_token = 1;
  bool finished = 2;
}

// (Optional) Full generation RPC for baseline target-only decoding.
message GenerateRequest {
}

message GenerateResponse {
  string output_text = 1;
}

// Speculative Decoding gRPC Service definition
service SpeculativeService {
  rpc StartGeneration(StartRequest) returns (StartResponse);
  rpc VerifyDraftTokens(VerifyRequest) returns (VerifyResponse);
  rpc FinalizeTokens(FinalizeRequest) returns (FinalizeResponse);
  rpc GenerateFull(StartRequest) returns (GenerateResponse);
}