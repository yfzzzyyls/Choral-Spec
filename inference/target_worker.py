import logging
import time
import torch
from concurrent import futures
import grpc
from inference import model_loader
from transformers import AutoTokenizer
from grpc_comm import inference_pb2, inference_pb2_grpc

logger = logging.getLogger(__name__)

class SpeculativeServiceServicer(inference_pb2_grpc.SpeculativeServiceServicer):
    def __init__(self, model_path, sequence_length=128):
        # Load or compile the target model and its tokenizer
        logger.info(f"Loading target model from '{model_path}' (sequence_length={sequence_length})...")
        self.model = model_loader.load_model(model_path, sequence_length=sequence_length)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        logger.info("Target model and tokenizer loaded.")
        # Generation state variables
        self.current_ids = None
        self.max_tokens = None
        self.tokens_generated = 0
        # End-of-sequence token ID for the model (if defined)
        self.eos_token_id = self.tokenizer.eos_token_id

    def StartGeneration(self, request, context):
        """Initialize generation with the given prompt and optional max token limit."""
        prompt_text = request.prompt
        max_tokens = request.max_new_tokens if request.max_new_tokens > 0 else None
        logger.info(f"StartGeneration called with prompt: \"{prompt_text}\", max_new_tokens: {max_tokens}")
        # Encode prompt into input IDs and reset state
        self.current_ids = self.tokenizer(prompt_text, return_tensors="pt").input_ids
        self.max_tokens = max_tokens
        self.tokens_generated = 0
        return inference_pb2.StartResponse(acknowledged=True)

    def VerifyDraftTokens(self, request, context):
        """Verify a chunk of draft-predicted tokens against the target model's next tokens."""
        draft_tokens = list(request.draft_tokens)
        logger.info(f"VerifyDraftTokens called with draft_tokens (IDs): {draft_tokens}")
        if self.current_ids is None:
            # No active generation session
            logger.error("No active generation context. Call StartGeneration first.")
            return inference_pb2.VerifyResponse(all_matched=False, match_count=0, correct_token=0, finished=True)
        # Number of tokens the draft model speculated
        num_tokens = len(draft_tokens)
        current_len = self.current_ids.shape[1]
        try:
            # Generate the next `num_tokens` tokens with the target model from the current context
            output = self.model.sample(self.current_ids, sequence_length=current_len + num_tokens)
        except Exception as e:
            logger.error(f"Target model generation failed: {e}")
            return inference_pb2.VerifyResponse(all_matched=False, match_count=0, correct_token=0, finished=True)
        # Extract the newly generated target tokens
        target_seq = output[0] if isinstance(output, (list, tuple)) else output[0]
        target_new_ids = target_seq[current_len:]
        target_new_ids = [int(t) for t in target_new_ids]
        logger.info(f"Target model predicted tokens (IDs): {target_new_ids}")
        # Compare draft tokens with target tokens
        match_count = 0
        all_matched = True
        correct_token_id = 0
        finished = False
        # If EOS was generated by target, note it and truncate sequence for comparison
        if self.eos_token_id is not None and self.eos_token_id in target_new_ids:
            eos_index = target_new_ids.index(self.eos_token_id)
            target_new_ids = target_new_ids[:eos_index + 1]
            finished = True  # target hit EOS
            logger.info(f"Target model generated EOS at position {eos_index} in the chunk.")
        # Compare token-by-token until a mismatch or one sequence ends
        for i in range(min(len(draft_tokens), len(target_new_ids))):
            if draft_tokens[i] == target_new_ids[i]:
                match_count += 1
            else:
                all_matched = False
                correct_token_id = target_new_ids[i]
                break
        else:
            # If no mismatch in the overlapped portion:
            if len(draft_tokens) == len(target_new_ids):
                all_matched = True
            else:
                # If lengths differ, treat as mismatch at end of shorter sequence
                all_matched = False
                match_count = min(len(draft_tokens), len(target_new_ids))
                if len(draft_tokens) > len(target_new_ids):
                    # Draft predicted more tokens, but target sequence ended
                    correct_token_id = 0  # no further token from target (target ended)
                    finished = True
                else:
                    # Target had an extra token (draft ended earlier, unlikely in normal flow)
                    correct_token_id = target_new_ids[len(draft_tokens)]
        # Update the target server's context with the accepted tokens
        if all_matched:
            # Accept all draft tokens (target and draft sequences matched)
            accepted_ids = target_new_ids  # same as draft_tokens when all matched
            if self.eos_token_id is not None and accepted_ids and accepted_ids[-1] == self.eos_token_id:
                finished = True  # If EOS is included, mark generation finished
        else:
            # Accept the matching prefix plus the target's correct token at first mismatch (if any)
            accepted_ids = []
            if match_count > 0:
                accepted_ids += target_new_ids[:match_count]
            if correct_token_id != 0:
                accepted_ids.append(correct_token_id)
        if accepted_ids:
            new_tokens_tensor = torch.tensor([accepted_ids], dtype=self.current_ids.dtype)
            self.current_ids = torch.cat([self.current_ids, new_tokens_tensor], dim=1)
            # Update count of tokens generated by target
            self.tokens_generated += len(accepted_ids)
        # If max_tokens was set and reached, mark completion
        if self.max_tokens is not None and self.tokens_generated >= self.max_tokens:
            finished = True
        # Build and return the response
        response = inference_pb2.VerifyResponse(all_matched=all_matched,
                                               match_count=match_count,
                                               correct_token=correct_token_id,
                                               finished=finished)
        logger.info(f"VerifyDraftTokens result: all_matched={all_matched}, match_count={match_count}, "
                    f"correct_token_id={correct_token_id}, finished={finished}")
        return response

    def GenerateFull(self, request, context):
        """Generate a full continuation for the given prompt using the target model (one-shot)."""
        prompt = request.prompt
        max_new_tokens = request.max_new_tokens
        logger.info(f"GenerateFull called with prompt: \"{prompt}\", max_new_tokens={max_new_tokens}")
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
        output = self.model.sample(input_ids, sequence_length=input_ids.shape[1] + max_new_tokens)
        seq = output[0] if isinstance(output, (list, tuple)) else output[0]
        gen_ids = seq[input_ids.shape[1]:]
        output_text = self.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()
        logger.info(f"GenerateFull returning text: \"{output_text}\"")
        return inference_pb2.GenerateResponse(output_text=output_text)

def run_server(model_path, port=50051, sequence_length=128, profile=False):
    """Launch the gRPC server hosting the target model for speculative decoding."""
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    servicer = SpeculativeServiceServicer(model_path, sequence_length=sequence_length)
    inference_pb2_grpc.add_SpeculativeServiceServicer_to_server(servicer, server)
    server.add_insecure_port(f"[::]:{port}")
    logger.info(f"Target server starting on port {port} (sequence_length={sequence_length})")
    server.start()
    server.wait_for_termination()

# (Optional) A local run function for target-only generation, used by main.py for profiling single-model performance
def run_local(model_path, prompt="", max_new_tokens=50, sequence_length=128, profile=False):
    """Run the target model locally (without gRPC) to generate text for a prompt."""
    logger.info("Running target model locally for output verification/profiling.")
    model = model_loader.load_model(model_path, sequence_length=sequence_length)
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids if prompt else torch.zeros((1,0), dtype=torch.long)
    output_text = ""
    tokens_generated = 0
    start_time = time.time() if profile else None
    for i in range(max_new_tokens):
        try:
            output = model.sample(input_ids, sequence_length=input_ids.shape[1] + 1)
        except Exception as e:
            logger.error(f"Target model generation failed: {e}")
            break
        token_id = int(output[0, -1]) if not isinstance(output, (list, tuple)) else int(output[0][-1])
        token_text = tokenizer.decode([token_id], clean_up_tokenization_spaces=True)
        print(f"Token {i+1}: {repr(token_text)}", flush=True)
        output_text += token_text
        # Append new token to input_ids for next iteration
        new_token_tensor = torch.tensor([[token_id]], dtype=input_ids.dtype)
        input_ids = torch.cat([input_ids, new_token_tensor], dim=1)
        tokens_generated += 1
        if tokenizer.eos_token_id is not None and token_id == tokenizer.eos_token_id:
            logger.info("EOS token encountered, stopping generation.")
            break
    # Profiling logs
    if profile and start_time is not None:
        total_time = time.time() - start_time
        throughput = tokens_generated / total_time if total_time > 0 else float('inf')
        logger.info(f"Target model generation completed in {total_time:.2f} seconds.")
        logger.info(f"Tokens generated: {tokens_generated}, Throughput: {throughput:.2f} tokens/sec")
        # Save performance metrics to CSV/JSON
        csv_file = f"performance_target_only_{time.strftime('%Y%m%d_%H%M%S')}.csv"
        json_file = csv_file.replace(".csv", ".json")
        try:
            with open(csv_file, "w") as cf:
                cf.write("total_latency,tokens_generated,throughput,avg_token_time,token_match_rate\n")
                avg_time = (total_time / tokens_generated) if tokens_generated > 0 else 0.0
                cf.write(f"{total_time:.6f},{tokens_generated},{throughput:.6f},{avg_time:.6f},N/A\n")
            metrics = {
                "total_latency": total_time,
                "tokens_generated": tokens_generated,
                "throughput": throughput,
                "token_match_rate": None
            }
            with open(json_file, "w") as jf:
                import json
                json.dump(metrics, jf, indent=2)
            logger.info(f"Performance metrics saved to {csv_file} and {json_file}")
        except Exception as e:
            logger.error(f"Failed to write performance metrics: {e}")
    print("\n=== Final Output ===\n" + (prompt + output_text))
    return output_text
