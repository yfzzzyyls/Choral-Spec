import random
from inference import speculative

class DummyTokenizer:
    def __init__(self, vocab_size=1000, eos_token_id=None):
        self.vocab_size = vocab_size
        self.eos_token_id = eos_token_id
    def decode(self, token_ids):
        # Convert a list of token IDs to a space-separated string for testing
        return " ".join(str(t) for t in token_ids)

class DummyDraftModel:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.call_count = 0
    def __call__(self, input_ids=None, use_cache=False, past_key_values=None):
        # Return logits that make certain tokens highly probable in sequence
        vocab = self.tokenizer.vocab_size
        logits = -1e9 * torch.ones((1, 1, vocab))
        if self.call_count == 0:
            # First token: token 10 very likely, token 11/12 somewhat likely
            logits[0, 0, 10] = 5.0
            logits[0, 0, 11] = 1.0
            logits[0, 0, 12] = 1.0
        elif self.call_count == 1:
            # Second token: token 11 very likely (to simulate a potential rejection scenario)
            logits[0, 0, 11] = 5.0
            logits[0, 0, 12] = 1.0
        else:
            # Third token (if reached): token 12 very likely
            logits[0, 0, 12] = 5.0
        self.call_count += 1
        # Create a dummy output object with required attributes
        return type('DummyOutput', (), {
            'logits': logits,
            'past_key_values': None
        })

class DummyVerifyResponse:
    def __init__(self, probs, finished):
        self.target_probs = probs
        self.finished = finished

class DummyFinalizeResponse:
    def __init__(self, token, finished):
        self.final_token = token
        self.finished = finished

class DummyStub:
    def __init__(self):
        self.verify_called = False
        self.finalize_called = False
        self.last_request_size = 0
        # Predefined behavior for target probabilities
        self.prob_map = {}  # not used in this simple dummy, we'll hardcode in method
    def VerifyDraftTokens(self, request):
        self.verify_called = True
        # Simulate target probabilities for each draft token
        draft_tokens = list(request.draft_tokens)
        self.last_request_size = len(draft_tokens)
        # Example: make token 10 highly probable (0.9), token 11 very low (0.05), token 12 moderate (0.5)
        probs = []
        for t in draft_tokens:
            if t == 10:
                probs.append(0.9)
            elif t == 11:
                probs.append(0.05)
            elif t == 12:
                probs.append(0.5)
            else:
                # default moderate probability
                probs.append(0.5)
        finished = False  # in this test, target doesn't finish during verify
        return DummyVerifyResponse(probs, finished)
    def FinalizeTokens(self, request):
        self.finalize_called = True
        accepted_count = request.accepted_count
        final_token = 0
        finished = False
        if accepted_count < self.last_request_size:
            # A rejection happened; provide a target token as replacement
            final_token = 13  # dummy token id generated by target
            finished = False
        # If accepted_count == last_request_size, no token needed (final_token remains 0)
        return DummyFinalizeResponse(final_token, finished)

def test_speculative_acceptance():
    random.seed(0)  # Seed random for reproducibility
    tokenizer = DummyTokenizer()
    draft_model = DummyDraftModel(tokenizer)
    stub = DummyStub()
    # Perform speculative decoding with dummy components
    output_text = speculative.speculative_decode(
        draft_model, tokenizer, stub,
        max_new_tokens=2, gamma=3, top_p=0.9
    )
    # The draft model would propose tokens [10, 11, ...]; target likely rejects token 11 and replaces with 13.
    # So the output sequence should be "10 13".
    assert output_text.strip() == "10 13"
    # Verify that gRPC calls were made
    assert stub.verify_called is True
    assert stub.finalize_called is True